import json
import random
from pathlib import Path

from dotenv import load_dotenv
import os

from openai import OpenAI  # ìƒˆ SDK


# ===== ê¸°ë³¸ ì„¤ì • =====

# ì´ íŒŒì¼: src/rag_rfp/eval/create_eval_dataset.py
# BASE_DIR: /rag-rfp-system
BASE_DIR = Path(__file__).resolve().parents[3]

CHUNKS_PATH = BASE_DIR / "data" / "processed" / "chunks_512_64_final.jsonl"
OUTPUT_PATH = BASE_DIR / "data" / "eval" / "rag_eval.jsonl"

NUM_SAMPLES = 30  # ëª‡ ê°œì˜ ì§ˆë¬¸/ì •ë‹µì„ ë§Œë“¤ì§€ (ì›í•˜ë©´ ë°”ê¿”ë„ ë¨)
MODEL_NAME = "gpt-5-mini"  # ê¶Œí•œ ìˆëŠ” ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥


# ===== OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” =====

load_dotenv(BASE_DIR / ".env")
api_key = os.getenv("OPENAI_API_KEY")
if api_key is None:
    raise RuntimeError("OPENAI_API_KEY not found. Check your .env at project root.")

client = OpenAI(api_key=api_key)


# ===== ìœ í‹¸ í•¨ìˆ˜ë“¤ =====

def load_chunks(path: Path):
    """
    chunks_512_64_final.jsonl ì€ JSONL í˜•ì‹:
      {"file": "...pdf", "chunk": "í…ìŠ¤íŠ¸..."}
      {"file": "...pdf", "chunk": "í…ìŠ¤íŠ¸..."}
      ...

    ì—¬ê¸°ì„œ:
      - id  : file ì´ë¦„ + ë¼ì¸ ì¸ë±ìŠ¤ë¥¼ ì¡°í•©í•´ì„œ ì‚¬ìš©
      - text: chunk í•„ë“œ ì‚¬ìš©
    """
    chunks = []
    with path.open("r", encoding="utf-8") as f:
        for idx, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                # í˜¹ì‹œ ì¤‘ê°„ì— ê¹¨ì§„ ì¤„ì´ ìˆì–´ë„ ì „ì²´ê°€ ì£½ì§€ ì•Šê²Œ ìŠ¤í‚µ
                # print("WARN: bad json line, skip")
                continue

            text = (
                obj.get("chunk")
                or obj.get("text")
                or obj.get("content")
            )
            if not text:
                continue

            file_name = obj.get("file", "")
            # file ëª… + ë¼ì¸ ì¸ë±ìŠ¤ë¡œ ê³ ìœ  id ìƒì„±
            chunk_id = file_name if file_name else str(idx)

            chunks.append({"id": chunk_id, "text": text})

    return chunks


def generate_qa_from_chunk(text: str):
    """
    í•œ ê°œì˜ chunk textë¥¼ ë°›ì•„ì„œ (question, answer)ë¥¼ ìƒì„±í•œë‹¤.
    chat.completions ê¸°ë°˜ ë²„ì „.
    """
    system_prompt = (
        "You are an assistant that creates evaluation questions for a Korean RFP document. "
        "You receive a single paragraph of the RFP and must create exactly one natural question "
        "a user might ask about this paragraph, and one ideal answer based ONLY on the paragraph."
    )

    user_prompt = f"""
ë‹¤ìŒì€ í•œêµ­ì–´ RFP(ì œì•ˆìš”ì²­ì„œ) ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

[ì»¨í…ìŠ¤íŠ¸]
{text}

ìœ„ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ,
1) ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ 1ê°œì™€
2) ê·¸ì— ëŒ€í•œ ëª¨ë²” ë‹µë³€ 1ê°œë¥¼ í•œêµ­ì–´ë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:

{{
  "question": "ì§ˆë¬¸ ë‚´ìš©",
  "answer": "ë‹µë³€ ë‚´ìš©"
}}
""".strip()

    # ğŸ”¥ responses.create ëŒ€ì‹  chat.completions.create ì‚¬ìš©
    resp = client.chat.completions.create(
        model=MODEL_NAME,   # "gpt-4o-mini" ë¡œ ì„¤ì •í•´ ë‘” ê°’
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    text_out = resp.choices[0].message.content.strip()

    # JSON íŒŒì‹±
    try:
        start = text_out.find("{")
        end = text_out.rfind("}")
        if start != -1 and end != -1:
            text_out = text_out[start : end + 1]

        obj = json.loads(text_out)
        question = obj.get("question", "").strip()
        answer = obj.get("answer", "").strip()
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback
        question = "ì´ ë¬¸ë‹¨ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ë©´ ë¬´ì—‡ì¸ê°€ìš”?"
        answer = text_out

    return question, answer


def main():
    # 1) chunks ë¡œë“œ
    print(f"Loading chunks from: {CHUNKS_PATH}")
    chunks = load_chunks(CHUNKS_PATH)
    print(f"Total chunks loaded: {len(chunks)}")

    if not chunks:
        print("No chunks loaded. Check CHUNKS_PATH and JSONL format.")
        return

    # 2) ìƒ˜í”Œë§
    num = min(NUM_SAMPLES, len(chunks))
    sampled = random.sample(chunks, num)
    print(f"Sampling {num} chunks for eval dataset generation.")

    # 3) ì¶œë ¥ í´ë” ìƒì„±
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    # 4) ê° chunkì—ì„œ Q/A ìƒì„± í›„ rag_eval.jsonlì— ê¸°ë¡
    with OUTPUT_PATH.open("w", encoding="utf-8") as f_out:
        for i, ch in enumerate(sampled, start=1):
            chunk_id = ch["id"]
            text = ch["text"]

            print(f"[{i}/{num}] Generating QA for chunk_id={chunk_id}...")

            question, answer = generate_qa_from_chunk(text)

            sample = {
                "id": f"q{i}",
                "question": question,
                "answer": answer,
                "relevant_chunk_ids": [chunk_id],
            }

            f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")

    print(f"\nDone! Saved eval dataset to: {OUTPUT_PATH}")
    print("ì´ì œ eval_retriever.py / eval_generator.py / eval_judge.py ë¥¼ ëŒë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()