import json
from pathlib import Path
from collections import defaultdict

from rag_rfp.retrieve.retriever import ChunkRetriever
 # ë„¤ í”„ë¡œì íŠ¸ì— ë§žê²Œ import

BASE_DIR = Path(__file__).resolve().parents[3]  # rag-rfp-system/
EVAL_PATH = BASE_DIR / "data" / "eval" / "rag_eval.jsonl"


def load_eval_dataset(path: Path):
    samples = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            samples.append(obj)
    return samples


def evaluate_retriever(k_values=(1, 3, 5)):
    # retriever ì´ˆê¸°í™” (ë„¤ retriever ì´ˆê¸°í™” ë°©ì‹ì— ë§žê²Œ ìˆ˜ì •í•´ë„ ë¨)
    retriever = ChunkRetriever()

    samples = load_eval_dataset(EVAL_PATH)
    print(f"Loaded {len(samples)} eval samples from {EVAL_PATH}")

    # kë³„ë¡œ hits/total ê³„ì‚°
    hits_at_k = defaultdict(int)
    total = len(samples)

    for sample in samples:
        question = sample["question"]
        gt_ids = set(sample["relevant_chunk_ids"])

        # ðŸ”´ ì—¬ê¸°ì„œ retriever ë©”ì„œë“œ ì´ë¦„ë§Œ ë„¤ ì½”ë“œì— ë§žê²Œ ë°”ê¾¸ë©´ ë¨
        # ì˜ˆ: retriever.search, retriever.retrieve, retriever.get_top_k ë“±
        results = retriever.search(question, top_k=max(k_values))

        # resultsê°€ [{"id": "...", "score": ...}, ...] í˜•íƒœë¼ê³  ê°€ì •
        retrieved_ids = [r["doc_id"] for r in results]

        for k in k_values:
            top_k_ids = set(retrieved_ids[:k])
            if gt_ids & top_k_ids:
                hits_at_k[k] += 1

    print("\nRetriever Evaluation Metrics:")
    for k in sorted(k_values):
        recall = hits_at_k[k] / total if total > 0 else 0.0
        print(f"Recall@{k}: {recall:.4f}")


if __name__ == "__main__":
    evaluate_retriever(k_values=(1, 3, 5))