import faiss
import numpy as np
import json
from openai import OpenAI
from tqdm import tqdm
from typing import List, Dict, Tuple
import os
from rank_bm25 import BM25Okapi
from konlpy.tag import Okt 
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from collections import defaultdict
import time # LLM í˜¸ì¶œ ì§€ì—° ë°©ì§€ìš©

# --- ì„¤ì •ê°’: ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš” ---
FAISS_INDEX_PATH = "/home/bhs1581/rag-rfp-system/chunking/vector_db/vectordb_multi_aspect_index.faiss" 
EVAL_JSONL_PATH = "/home/bhs1581/rag-rfp-system/chunking/eval/our_clean_eval_style.jsonl" 
CHUNK_FILE_PATH = "/home/bhs1581/rag-rfp-system/chunking/chunks/chunks_multi_aspect (1).jsonl"
# ðŸš¨ ì¿¼ë¦¬ ë³€í™˜ ê²°ê³¼ë¥¼ ì €ìž¥í•  ìºì‹œ íŒŒì¼ ê²½ë¡œ
QUERY_CACHE_PATH = "./rewritten_queries_cache.json" 

# --- Reranker ì„¤ì • ---
RERANKER_MODEL_NAME = "BAAI/bge-reranker-base"  
RERANK_TOP_K = 10 

# --- ì„±ëŠ¥ ì¸¡ì • ìƒìˆ˜ ---
TARGET_DIMENSION = 1536 
MAX_K_DENSE = 20        
MAX_K_SPARSE = 20       
HYBRID_K = 20           
RRF_K = 60              # ìµœì  ì„±ëŠ¥ K=60 ê³ ì •
TEST_PREFIX = "RFPì˜ í•„ìˆ˜ ì •ë³´ë¥¼ ì°¾ê³  ìžˆìŠµë‹ˆë‹¤: " 

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY")) 
except Exception as e:
    print(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    exit()

# --- ì „ì—­ ë³€ìˆ˜ ë° ëª¨ë¸ ì´ˆê¸°í™” ---
ALL_CHUNK_TEXTS: List[str] = []
BM25_MODEL = None
OKT = Okt() 
QUERY_CACHE: Dict[str, List[str]] = {} # ì¿¼ë¦¬ ë³€í™˜ ê²°ê³¼ë¥¼ ì €ìž¥í•  ë©”ëª¨ë¦¬ ìºì‹œ

try:
    print(f"BGE Reranker Base ëª¨ë¸({RERANKER_MODEL_NAME}) ë¡œë“œë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")
    RERANKER_TOKENIZER = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)
    RERANKER_MODEL = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME)
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    RERANKER_MODEL.to(DEVICE)
    print(f"âœ… Reranker ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {RERANKER_MODEL_NAME} (Device: {DEVICE})")
except Exception as e:
    print(f"âŒ Reranker ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {RERANKER_MODEL_NAME} ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ.")
    print(f"ìƒì„¸ ì˜¤ë¥˜: {e}")
    exit()

# --- 1. ë°ì´í„° ë¡œë“œ ë° BM25 ê°ì²´ ìƒì„± (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
def load_chunk_data(chunk_file_path: str) -> Dict[int, str]:
    global ALL_CHUNK_TEXTS, BM25_MODEL
    mapping = {}
    with open(chunk_file_path, 'r', encoding='utf-8') as f:
        chunks = [json.loads(line) for line in f]
    for index, chunk in enumerate(chunks):
        mapping[index] = chunk['doc_id'] 
        ALL_CHUNK_TEXTS.append(chunk['text'])
    print(f"ì²­í¬-DocID ë§¤í•‘ í…Œì´ë¸” ìƒì„± ì™„ë£Œ. ì´ {len(mapping)}ê°œ í•­ëª©.")
    tokenized_corpus = []
    for doc in tqdm(ALL_CHUNK_TEXTS, desc="BM25 Corpus Tokenizing (Okt - Nouns)"):
        tokens = OKT.nouns(doc)
        tokenized_corpus.append(tokens)
    BM25_MODEL = BM25Okapi(tokenized_corpus)
    print("âœ… BM25 ëª¨ë¸ (Okt ëª…ì‚¬ ì¶”ì¶œ ì ìš©) ì´ˆê¸°í™” ì™„ë£Œ.")
    return mapping

# --- 2. ìž„ë² ë”© ë° FAISS ë¡œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
def load_faiss_index(path: str):
    try:
        index = faiss.read_index(path)
        if index.d != TARGET_DIMENSION:
            print(f"âŒ ê²½ê³ : ë¡œë“œëœ ì¸ë±ìŠ¤ ì°¨ì›({index.d})ì´ ëª©í‘œ ì°¨ì›({TARGET_DIMENSION})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
        print(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ: {path} (ì´ {index.ntotal}ê°œ ë²¡í„°)")
        return index
    except Exception as e:
        print(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_query_embeddings(queries: List[str]) -> np.ndarray:
    OPENAI_EMBEDDING_MODEL = "text-embedding-3-small"
    response = client.embeddings.create(input=queries, model=OPENAI_EMBEDDING_MODEL, dimensions=TARGET_DIMENSION)
    embeddings = [data.embedding for data in response.data]
    return np.array(embeddings, dtype=np.float32)

# --- ì‹ ê·œ ìºì‹œ ê´€ë¦¬ í•¨ìˆ˜ ---
def load_query_cache(path: str, eval_data_questions: List[str]) -> Dict[str, List[str]]:
    """ ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤. íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¶ˆì™„ì „í•˜ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    cache = {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            cache_list = json.load(f)
            # ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ì›ë³¸ ì¿¼ë¦¬: ë³€í™˜ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸)
            for item in cache_list:
                cache[item['original_query']] = item['rewritten_queries']
        
        # ìºì‹œì˜ ì™„ì „ì„± ê²€ì‚¬
        if len(cache) == len(eval_data_questions):
            print(f"âœ… Query Rewriting Cache ë¡œë“œ ì„±ê³µ: {len(cache)}ê°œ ì¿¼ë¦¬ (ì™„ì „)")
        else:
            print(f"âš ï¸ Query Rewriting Cacheê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ìž¬êµ¬ì„±ì„ ì‹œë„í•©ë‹ˆë‹¤. (ìš”ì²­ {len(eval_data_questions)}ê°œ, ìºì‹œ {len(cache)}ê°œ)")
            # ë¶ˆì™„ì „í•œ ìºì‹œë¥¼ ì‚¬ìš©í•˜ë©´ ë³€ë™ì„±ì„ ìœ ë°œí•˜ë¯€ë¡œ ë‹¤ì‹œ LLM í˜¸ì¶œì„ ìœ ë„
            return {} 
        return cache
    except (FileNotFoundError, json.JSONDecodeError):
        print(f"âš ï¸ Query Rewriting Cache íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ìž˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {path}. LLM í˜¸ì¶œì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        return {}

def save_query_cache(path: str, cache: Dict[str, List[str]]):
    """ í˜„ìž¬ ë©”ëª¨ë¦¬ ìºì‹œë¥¼ íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤. """
    cache_list = [{'original_query': q, 'rewritten_queries': r} for q, r in cache.items()]
    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(cache_list, f, ensure_ascii=False, indent=4)
        print(f"ðŸ’¾ Query Rewriting Cache íŒŒì¼ ì €ìž¥ ì„±ê³µ: {path}")
    except Exception as e:
        print(f"âŒ Query Rewriting Cache íŒŒì¼ ì €ìž¥ ì‹¤íŒ¨: {e}")

# --- 3. Multi-step Query Transformation (ìºì‹œ ì ìš©) ---
def transform_query(query: str) -> List[str]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œì˜ ëª…í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ìºì‹œê°€ ìžˆë‹¤ë©´ ìºì‹œë¥¼ ì‚¬ìš©í•˜ê³ , ì—†ë‹¤ë©´ LLMì„ í˜¸ì¶œí•˜ì—¬ ìºì‹œë¥¼ ì±„ì›ë‹ˆë‹¤.
    """
    global QUERY_CACHE
    
    # 1. ìºì‹œ ížˆíŠ¸ ì²´í¬
    if query in QUERY_CACHE:
        return QUERY_CACHE[query]

    # 2. ìºì‹œ ë¯¸ìŠ¤ ì‹œ LLM í˜¸ì¶œ
    print(f"   [LLM í˜¸ì¶œ] {query} -> Rewriting...")
    prompt = f"""
    ë‹¹ì‹ ì€ RFP ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì „ë¬¸ ê²€ìƒ‰ì–´ ìƒì„±ê¸°ìž…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ì‚¬ìš©ìž ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬, RFP ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìžˆëŠ” 3ê°€ì§€ì˜ ë…ë¦½ì ì´ê³  ëª…í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
    ê²°ê³¼ëŠ” ì˜¤ì§ ì¿¼ë¦¬ 3ê°œë§Œ, ê° ì¤„ì— í•˜ë‚˜ì”© ë‚˜ì—´ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¬¸ìž¥ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    
    ì‚¬ìš©ìž ì¿¼ë¦¬: "{query}"
    """
    try:
        # Rate Limit íšŒí”¼ë¥¼ ìœ„í•´ ìž ì‹œ ëŒ€ê¸°
        time.sleep(0.5) 
        
        response = client.chat.completions.create(
            model="gpt-4o-mini", 
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        transformed_queries = response.choices[0].message.content.strip().split('\n')
        
        valid_queries = [q.strip() for q in transformed_queries if q.strip()]
        if not valid_queries:
            valid_queries = [query]
            
        # LLM í˜¸ì¶œ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ìž¥ (ë©”ëª¨ë¦¬)
        QUERY_CACHE[query] = valid_queries 
        return valid_queries
    
    except Exception as e:
        print(f"âŒ ì¿¼ë¦¬ ë³€í™˜ ì‹¤íŒ¨ (LLM í˜¸ì¶œ): {e}")
        return [query]

# --- 4. Hybrid Search í•µì‹¬ ë¡œì§ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
def dense_search(index: faiss.Index, query_vector: np.ndarray, top_k: int) -> Dict[int, float]:
    faiss.normalize_L2(query_vector.reshape(1, -1))
    D, I = index.search(query_vector.reshape(1, -1), top_k)
    results = {}
    for rank, idx in enumerate(I[0]):
        if idx >= 0:
            results[int(idx)] = rank + 1
    return results

def sparse_search(query: str, top_k: int) -> Dict[int, float]:
    tokenized_query = OKT.nouns(query) 
    scores = BM25_MODEL.get_scores(tokenized_query)
    ranked_indices = np.argsort(scores)[::-1]
    results = {}
    for rank, idx in enumerate(ranked_indices[:top_k]):
        results[int(idx)] = rank + 1
    return results

def reciprocal_rank_fusion(all_ranks: List[Dict[int, float]], k: int = RRF_K) -> List[int]:
    fused_scores = defaultdict(float)
    for ranks in all_ranks:
        for index, rank in ranks.items():
            score = 1.0 / (k + rank)
            fused_scores[index] += score
            
    sorted_indices = sorted(fused_scores.keys(), key=lambda idx: fused_scores[idx], reverse=True)
    return sorted_indices[:HYBRID_K]

def multi_step_hybrid_search(index: faiss.Index, original_query: str) -> List[int]:
    
    # 1. ì¿¼ë¦¬ ë³€í™˜ (ìºì‹œ ì‚¬ìš©)
    transformed_queries = transform_query(original_query)
    
    all_ranks: List[Dict[int, float]] = []

    for query in transformed_queries:
        dense_query = f"{TEST_PREFIX}{query}"
        query_vector = get_query_embeddings([dense_query])[0]
        dense_ranks = dense_search(index, query_vector, MAX_K_DENSE)
        sparse_ranks = sparse_search(query, MAX_K_SPARSE)
        
        all_ranks.append(dense_ranks)
        all_ranks.append(sparse_ranks)

    # 5. RRF í†µí•©
    fused_indices = reciprocal_rank_fusion(all_ranks)
    return fused_indices

# --- 5. Reranker ë¡œì§ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
def rerank_results(query: str, retrieved_indices: List[int], chunk_mapping: Dict[int, str], top_k: int) -> List[int]:
    pairs = []
    for idx in retrieved_indices:
        chunk_text = ALL_CHUNK_TEXTS[idx]
        pairs.append([query, chunk_text])
        
    if not pairs: return []

    inputs = RERANKER_TOKENIZER(pairs, padding=True, truncation=True, return_tensors='pt').to(DEVICE)

    with torch.no_grad():
        RERANKER_MODEL.eval()
        outputs = RERANKER_MODEL(**inputs)
        scores = outputs.logits.squeeze(dim=1).cpu().numpy()
    
    indexed_scores = list(zip(retrieved_indices, scores))
    indexed_scores.sort(key=lambda item: item[1], reverse=True)
    reranked_indices = [idx for idx, score in indexed_scores]
    
    return reranked_indices[:top_k]

# --- 6. Multi-step Reranked Recall ê³„ì‚° í•¨ìˆ˜ (R@10 í¬í•¨) ---
def evaluate_reranked_recall(
    index: faiss.Index, eval_data: List[Dict], chunk_mapping: Dict[int, str], max_k: int = RERANK_TOP_K
) -> Dict[str, float]:
    
    k_list = sorted([k for k in [1, 3, 5, 10] if k <= max_k]) 
    results = {f"R@{k}": 0 for k in k_list}
    total_queries = len(eval_data)
    
    gt_doc_ids = [item['gt_doc_id'] for item in eval_data]
    original_queries = [item['question'] for item in eval_data] 

    for q_idx in tqdm(range(total_queries), desc="Multi-step Reranked Recall í‰ê°€ ì§„í–‰ ì¤‘"):
        query_text = original_queries[q_idx] 
        ground_truth_doc = gt_doc_ids[q_idx]
        
        retrieved_indices_hybrid = multi_step_hybrid_search(index, query_text)
        reranked_indices = rerank_results(query_text, retrieved_indices_hybrid, chunk_mapping, top_k=RERANK_TOP_K)
        
        retrieved_docs = [chunk_mapping.get(idx) for idx in reranked_indices if idx != -1] 
        
        for k in k_list:
            if ground_truth_doc in retrieved_docs[:k]:
                results[f"R@{k}"] += 1
                
    for k in results.keys():
        results[k] /= total_queries
        
    return results

# --- 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìºì‹œ ê´€ë¦¬ ë¡œì§ ì¶”ê°€) ---

if __name__ == "__main__":
    
    print("\n===================================================")
    print(f"ðŸš€ ìµœì¢… ì‹œìŠ¤í…œ í‰ê°€ (Multi-step Hybrid + Reranker Base) - RRF K={RRF_K}")
    print("===================================================")
    
    index = load_faiss_index(FAISS_INDEX_PATH)
    if index is None: exit()
    
    try:
        with open(EVAL_JSONL_PATH, 'r', encoding='utf-8') as f:
            eval_data = [json.loads(line) for line in f]
        print(f"í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: ì´ {len(eval_data)}ê°œ ì¿¼ë¦¬")
    except Exception as e:
        print(f"âŒ í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {EVAL_JSONL_PATH} íŒŒì¼ í™•ì¸ í•„ìš”. {e}")
        exit()
        
    try:
        chunk_mapping = load_chunk_data(CHUNK_FILE_PATH)
    except Exception as e:
        print(f"âŒ ì²­í¬ íŒŒì¼ ë¡œë“œ ë° BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {CHUNK_FILE_PATH} íŒŒì¼ ê²½ë¡œ/í˜•ì‹ í™•ì¸ í•„ìš”. {e}")
        exit()

    # ðŸš¨ ì¿¼ë¦¬ ìºì‹œ ë¡œë“œ ë° LLM í˜¸ì¶œ/ìºì‹œ ìƒì„± ë¡œì§ 
    eval_questions = [item['question'] for item in eval_data]
    QUERY_CACHE = load_query_cache(QUERY_CACHE_PATH, eval_questions)

    print("\n---------------------------------------------------")
    print(f"   - 1ì°¨ ê²€ìƒ‰: Multi-step Hybrid Search (RRF K={RRF_K}, Top {HYBRID_K})")
    print(f"   - 2ì°¨ ìˆœìœ„ ì¡°ì •: {RERANKER_MODEL_NAME} (Top {RERANK_TOP_K})")
    print("---------------------------------------------------")

    final_results = evaluate_reranked_recall(
        index, 
        eval_data, 
        chunk_mapping, 
        max_k=RERANK_TOP_K
    )
    
    # ðŸš¨ LLM í˜¸ì¶œì´ ì´ë£¨ì–´ì¡Œê³ , ìºì‹œ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° (ìµœì´ˆ ì‹¤í–‰) ì €ìž¥
    if not os.path.exists(QUERY_CACHE_PATH) and len(QUERY_CACHE) == len(eval_questions):
        print("\nìµœì´ˆ ì‹¤í–‰ ì™„ë£Œ. ì„±ëŠ¥ ë³€ë™ì„ ë§‰ê¸° ìœ„í•´ ì¿¼ë¦¬ ë³€í™˜ ê²°ê³¼ë¥¼ ìºì‹±í•©ë‹ˆë‹¤.")
        save_query_cache(QUERY_CACHE_PATH, QUERY_CACHE)
        
    print("\n---------------------------------------------------")
    print("ðŸŒŸ Multi-step Hybrid + Reranker Base ìµœì¢… ì„±ëŠ¥:")
    for k, score in final_results.items():
        print(f"{k}: {score:.3f}")
    print("===================================================")