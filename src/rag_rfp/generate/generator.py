# src/rag_rfp/generate/generator.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer

# ğŸ”¹ íŒ€ì›ì´ ë„˜ê²¨ì¤€ retriever.py (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì½”ë“œ) ë¶ˆëŸ¬ì˜¤ê¸°
#   ì˜ˆ: í”„ë¡œì íŠ¸ êµ¬ì¡°ê°€ src/rag_rfp/retrieve/retriever.py ë¼ê³  ê°€ì •
from rag_rfp.retrieve import retriever as hybrid_retriever


@dataclass
class RAGAnswer:
    """LLM ë‹µë³€ + ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” êµ¬ì¡°ì²´."""
    answer: str
    contexts: List[Dict[str, Any]]


class RAGGenerator:
    """
    RAG Generator (Gemma 2-2B-IT ê¸°ë°˜)

    ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•œë‹¤.

    1) ì™¸ë¶€ retriever ì£¼ì… ëª¨ë“œ
       - __init__(retriever=ì™¸ë¶€_retriever, ...)
       - ì™¸ë¶€ retrieverëŠ” .retrieve(question, top_k) ë©”ì„œë“œë¥¼ ê°€ì ¸ì•¼ í•œë‹¤.

    2) í•˜ì´ë¸Œë¦¬ë“œ ë‚´ì¥ ëª¨ë“œ (ì§€ê¸ˆ íŒ€ì›ì´ ì¤€ retriever.py ì‚¬ìš©)
       - __init__(retriever=None, ...)
       - bge-m3 + FAISS + BM25 + RRF (hybrid_search) ì‚¬ìš©
       - retriever.py ì˜ ì„¤ì •ê°’(CHUNK_FILE_PATH, FAISS_INDEX_PATH ë“±)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    """

    def __init__(
        self,
        retriever: Optional[object] = None,
        model_id: str = "google/gemma-2-2b-it",  # Gemma 2-2B-IT
        top_k: int = 5,
        device: str = "cuda",
    ):
        self.external_retriever = retriever  # Noneì´ë©´ í•˜ì´ë¸Œë¦¬ë“œ ë‚´ì¥ ëª¨ë“œ
        self.top_k = top_k

        # GPU ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ cpuë¡œ fallback
        if device == "cuda" and not torch.cuda.is_available():
            print("[RAGGenerator] CUDA ë¯¸ì‚¬ìš© í™˜ê²½ ê°ì§€ â†’ device='cpu'ë¡œ ë³€ê²½")
            device = "cpu"
        self.device = device

        # ==========================================
        # 1) Gemma 2-2B-IT ëª¨ë¸ / í† í¬ë‚˜ì´ì € ë¡œë“œ
        # ==========================================
        print("[RAGGenerator] Loading Gemma-2-2B-IT...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            padding_side="left",
            trust_remote_code=True,
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=True,
        )

        if self.device != "cuda":
            # device_map=None ì¸ ê²½ìš° ì§ì ‘ to(device)
            self.model.to(self.device)

        # ==========================================
        # 2) retriever ëª¨ë“œ ì„¤ì •
        #    - ì™¸ë¶€ retriever ê°€ ì—†ìœ¼ë©´, í•˜ì´ë¸Œë¦¬ë“œ ë‚´ì¥ ëª¨ë“œ ì´ˆê¸°í™”
        # ==========================================
        if self.external_retriever is None:
            # í•˜ì´ë¸Œë¦¬ë“œ ë‚´ì¥ ëª¨ë“œ
            print("[RAGGenerator] ì™¸ë¶€ retriever ë¯¸ì§€ì • â†’ í•˜ì´ë¸Œë¦¬ë“œ ë‚´ì¥ ëª¨ë“œ í™œì„±í™”")
            self._init_hybrid_retriever()
        else:
            print("[RAGGenerator] ì™¸ë¶€ retriever ì‚¬ìš© ëª¨ë“œ í™œì„±í™”")

    # ==========================================================
    # í•˜ì´ë¸Œë¦¬ë“œ retriever ì´ˆê¸°í™” (íŒ€ì› retriever.py ê¸°ë°˜)
    # ==========================================================
    def _init_hybrid_retriever(self):
        """
        íŒ€ì›ì´ ë„˜ê²¨ì¤€ retriever.py ì— ì •ì˜ëœ ì„¤ì •ê³¼ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ
        bge-m3 + FAISS + BM25 + RRF í™˜ê²½ì„ ì´ˆê¸°í™”í•œë‹¤.
        """
        print("[RAGGenerator] Loading bge-m3 model for hybrid retrieval...")
        self.hybrid_model = SentenceTransformer(
            "BAAI/bge-m3",
            trust_remote_code=True,
        )
        self.hybrid_model = self.hybrid_model.to(self.device)

        print("[RAGGenerator] Loading chunks and BM25 (retriever.load_chunk_mapping)...")
        # ì´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ BM25_MODEL ì „ì—­ë³€ìˆ˜ë„ ì´ˆê¸°í™”ë¨
        self.chunk_mapping, self.chunks = hybrid_retriever.load_chunk_mapping(
            hybrid_retriever.CHUNK_FILE_PATH
        )

        print("[RAGGenerator] Loading FAISS index (retriever.load_faiss_index)...")
        self.faiss_index = hybrid_retriever.load_faiss_index(
            hybrid_retriever.FAISS_INDEX_PATH
        )

        print("[RAGGenerator] Hybrid retriever initialization complete.")

    # ==========================================================
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â†’ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    # ==========================================================
    def _hybrid_retrieve(self, question: str, top_k: int) -> List[Dict[str, Any]]:
        """
        íŒ€ì›ì´ ì¤€ hybrid_search(model, index, query)ë¥¼ í˜¸ì¶œí•´ì„œ
        ìƒìœ„ chunk ì¸ë±ìŠ¤ë¥¼ ì–»ê³ , ì´ë¥¼ ì»¨í…ìŠ¤íŠ¸ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
        """
        # hybrid_retriever.HYBRID_TOP_K ë§Œí¼ ë½‘ê³  ê·¸ ì¤‘ ìƒìœ„ top_kë§Œ ì‚¬ìš©
        indices = hybrid_retriever.hybrid_search(
            self.hybrid_model,
            self.faiss_index,
            question,
        )[:top_k]

        contexts: List[Dict[str, Any]] = []
        for idx in indices:
            chunk = self.chunks[idx]  # load_chunk_mapping ì—ì„œ ì½ì–´ì˜¨ ì›ë³¸ chunk dict

            # chunk ì˜ êµ¬ì¡°ë¥¼ ê°€ì •:
            # {
            #   "doc_id": ...,
            #   "text": ...,
            #   "chunk_index": ... (ìˆì„ ìˆ˜ë„ ìˆê³  ì—†ì„ ìˆ˜ë„ ìˆìŒ)
            #   ...
            # }
            doc_id = chunk.get("doc_id")
            chunk_index = chunk.get("chunk_index", idx)

            ctx = {
                "text": chunk.get("text", ""),
                "doc_id": doc_id,
                "chunk_id": idx,
                "meta": {
                    "doc_id": doc_id,
                    "chunk_index": chunk_index,
                },
            }
            contexts.append(ctx)

        return contexts

    # ==========================================================
    # ë©”ì¸ ì§„ì…ì : ì§ˆë¬¸ â†’ RAGAnswer
    # ==========================================================
    def ask(self, question: str, top_k: Optional[int] = None) -> RAGAnswer:
        """ì§ˆë¬¸ì„ ë°›ì•„ RAGAnswer ë°˜í™˜."""
        k = top_k or self.top_k

        # 1) ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        if self.external_retriever is not None:
            # ì™¸ë¶€ retriever ì‚¬ìš©
            contexts = self.external_retriever.retrieve(question, top_k=k)
        else:
            # í•˜ì´ë¸Œë¦¬ë“œ ë‚´ì¥ ëª¨ë“œ ì‚¬ìš©
            contexts = self._hybrid_retrieve(question, top_k=k)

        # 2) ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±
        def _format_header(c: Dict[str, Any]) -> str:
            """
            doc_id / chunk_index / chunk_id ë¥¼ ìƒí™©ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ í‘œì‹œ.
            """
            meta = c.get("meta", {}) or {}
            doc_id = meta.get("doc_id", c.get("doc_id", "unknown_doc"))
            chunk_index = meta.get(
                "chunk_index",
                c.get("chunk_index", c.get("chunk_id", "unknown_chunk")),
            )
            return f"[doc={doc_id}, chunk={chunk_index}]"

        context_text = "\n\n".join(
            f"{_format_header(c)}\n{c.get('text', '')}"
            for c in contexts
        )

        # 3) Gemmaì—ì„œ ì˜ ë™ì‘í•˜ëŠ” ë‹¨ì¼ prompt êµ¬ì„±
        prompt = (
            "ë„ˆëŠ” ê³µê³µ RFP ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\n"
            "ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì•ˆì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•´.\n"
            "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆ.\n\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸:\n{question}\n\n"
            f"ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸:\n{context_text}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.\n"
            "ê°€ëŠ¥í•˜ë©´ bullet í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì¤˜."
        )

        # 4) Gemma ëª¨ë¸ ì‹¤í–‰
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
        ).to(self.device)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.3,      # RAGëŠ” ë‚®ê²Œ ìœ ì§€ (í™˜ê° ë°©ì§€)
            do_sample=False,      # ì¬í˜„ì„± & ì •í™•ë„ ìš°ì„ 
        )

        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # í”„ë¡¬í”„íŠ¸ê¹Œì§€ í¬í•¨ëœ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ, ì‹¤ì œ ë‹µë³€ ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ê¸°
        if full_text.startswith(prompt):
            answer = full_text[len(prompt):].strip()
        else:
            # í˜¹ì‹œ tokenizer ì²˜ë¦¬ë¡œ promptê°€ ì•½ê°„ ë‹¬ë¼ì¡Œë‹¤ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
            answer = full_text.strip()

        return RAGAnswer(answer=answer, contexts=contexts)

    def __call__(self, question: str, top_k: Optional[int] = None) -> RAGAnswer:
        """
        generator(question) í˜•íƒœë¡œë„ ì“¸ ìˆ˜ ìˆê²Œ í˜¸ì¶œ ì—°ì‚°ì ì˜¤ë²„ë¼ì´ë“œ.
        (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ìš©)
        """
        return self.ask(question, top_k=top_k)


# ğŸ” ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
Generator = RAGGenerator