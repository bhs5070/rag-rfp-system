# src/rag_rfp/generate/generator.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Any

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


@dataclass
class RAGAnswer:
    """LLM ë‹µë³€ + ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” êµ¬ì¡°ì²´."""
    answer: str
    contexts: List[Dict[str, Any]]


class RAGGenerator:
    """
    Retriever â†’ Gemma-2-2B-IT ì¡°í•©ì˜ ì˜¨í”„ë ˆë¯¸ìŠ¤ RAG Generator.
    """

    def __init__(
        self,
        retriever,
        model_id: str = "google/gemma-2-2b-it",  # â† ì •í™•í•˜ê²Œ ì •ì •
        top_k: int = 5,
        device: str = "cuda"
    ):
        self.retriever = retriever
        self.top_k = top_k
        self.device = device

        # ğŸ”¹ Gemma 2-2B-IT ëª¨ë¸ / í† í¬ë‚˜ì´ì € ë¡œë“œ
        #    - padding_side="left" ê¶Œì¥
        #    - trust_remote_code=True í•„ìš” (Gemmaì˜ HF ëª¨ë¸ êµ¬ì¡° ë•Œë¬¸)
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            padding_side="left",
            trust_remote_code=True
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,        # L4 GPUì— ìµœì 
            device_map="auto",                # ìë™ GPU ë§¤í•‘
            trust_remote_code=True
        )

    def ask(self, question: str, top_k: int | None = None) -> RAGAnswer:
        k = top_k or self.top_k

        # 1) Retrieverë¡œ top-k chunk ê²€ìƒ‰
        contexts = self.retriever.search(question, top_k=k)

        # 2) ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±
        context_text = "\n\n".join(
            [
                f"[doc={c.get('doc_id')}, chunk={c.get('chunk_index')}]\n{c['text']}"
                for c in contexts
            ]
        )

        # 3) Gemmaì—ì„œ ì˜ ë™ì‘í•˜ëŠ” ë‹¨ì¼ prompt êµ¬ì„±
        prompt = (
            "ë„ˆëŠ” ê³µê³µ RFP ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\n"
            "ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì•ˆì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•´.\n"
            "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆ.\n\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸:\n{question}\n\n"
            f"ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸:\n{context_text}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.\n"
            "ê°€ëŠ¥í•˜ë©´ bullet í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì¤˜."
        )

        # 4) Gemma ëª¨ë¸ ì‹¤í–‰
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.3,      # RAGëŠ” ë‚®ê²Œ ìœ ì§€ (í™˜ê° ë°©ì§€)
            do_sample=False,      # ì¬í˜„ì„± & ì •í™•ë„ ìš°ì„  (ê¶Œì¥)
        )

        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        return RAGAnswer(answer=answer, contexts=contexts)
