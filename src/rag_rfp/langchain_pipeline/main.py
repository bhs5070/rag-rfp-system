import os
import json
from typing import Dict, List

from retriever_core import RFPRetrieverCore
from lc_custom_retriever import CustomRFPRetriever
from evaluate_generator import evaluate_one  # /eval ì—ì„œ ì‚¬ìš©

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

import faiss
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification


###########################################
# CONFIG
###########################################
FAISS_INDEX_PATH = "/home/bhs1581/rag-rfp-system/chunking/vector_db/vectordb_multi_aspect_index.faiss"
CHUNK_PATH = "/home/bhs1581/rag-rfp-system/chunking/chunks/chunks_multi_aspect (1).jsonl"

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]


###########################################
# LOAD INDEX + CHUNKS
###########################################

def load_faiss(path):
    idx = faiss.read_index(path)
    print(f"âœ… FAISS loaded: {idx.ntotal} vectors")
    return idx

def load_chunks(path):
    texts, mapping = [], {}
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            obj = json.loads(line)
            texts.append(obj["text"])
            mapping[i] = obj["doc_id"]
    print(f"âœ… Loaded {len(texts)} chunks")
    return texts, mapping


print("\n=== Loading Vector DB ===")
index = load_faiss(FAISS_INDEX_PATH)
chunk_texts, chunk_map = load_chunks(CHUNK_PATH)


###########################################
# LOAD RERANKER
###########################################

print("\n=== Loading Reranker ===")
reranker_model_name = "BAAI/bge-reranker-base"
tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)
model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"âœ… Reranker on {device}")


###########################################
# INIT RETRIEVAL CORE
###########################################

core = RFPRetrieverCore(
    faiss_index=index,
    chunk_texts=chunk_texts,
    chunk_mapping=chunk_map,
    openai_api_key=OPENAI_API_KEY,
    reranker_model=model,
    reranker_tokenizer=tokenizer,
    device=device,
)
print("âœ… RetrieverCore ready\n")


###########################################
# LANGCHAIN RETRIEVER WRAPPER
###########################################

retriever = CustomRFPRetriever(
    core=core,
    is_multistep=True,
    top_k=10
)


###########################################
# LLM PIPELINE
###########################################

llm = ChatOpenAI(
    model="gpt-5-mini",
)

prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ í•œêµ­ RFP ë¬¸ì„œ ê¸°ë°˜ QA ëª¨ë¸ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ contextë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

<context>
{context}
</context>

ì§ˆë¬¸: {question}

ë¬¸ì„œì— ì •ë³´ê°€ ì—†ë‹¤ë©´ ì¶œë ¥:
"í•´ë‹¹ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
""")

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

print("âœ… RAG pipeline assembled\n")


###########################################
# INPUT SANITIZER
###########################################

def clean_input(t):
    return (
        t.encode("utf-8", "ignore")
         .decode("utf-8", "ignore")
         .replace("\ufffd", "")
         .strip()
    )


###########################################
# RERANK SCORE THRESHOLD
###########################################
THRESHOLD = -3.3


###########################################
# INTERACTIVE LOOP (WITH /eval SUPPORT)
###########################################

def interactive_loop():
    print("=== Intelligent RAG System Started ===")
    print("ğŸ’¬ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit):")

    while True:
        raw = input("\nğŸ” Query: ").strip()
        query = clean_input(raw)

        if query.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ----------------------------------------------------
        #  ğŸ“Œ /eval ëª¨ë“œ (generator í‰ê°€)
        # ----------------------------------------------------
        if query.startswith("/eval"):
            real_q = clean_input(query.replace("/eval", "").strip())

            print("\nğŸ“Š Evaluating generator...\n")

            # 1) Retrieval
            docs = retriever._get_relevant_documents(real_q)
            rerank_score = core.last_rerank_score

            # 2) Generation
            answer = rag_chain.invoke(real_q).content

            # 3) í‰ê°€ ì‹¤í–‰
            result = evaluate_one(
                question=real_q,
                answer=answer,
                retrieved_docs=docs
            )

            print("=== Evaluation Result ===")
            for k, v in result.items():
                print(f"{k}: {v}")
            print("=========================\n")
            continue
        # ----------------------------------------------------

        # ----------------------------------------------------
        # ğŸ”¥ ì¼ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬
        # ----------------------------------------------------
        print("\nâ³ Retrieving...\n")

        docs = retriever._get_relevant_documents(query)
        rerank_score = core.last_rerank_score

        # threshold fallback
        if len(docs) == 0 or rerank_score < THRESHOLD:
            print("ğŸ§  Answer:")
            print("í•´ë‹¹ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue

        print("â³ Generating answer...\n")
        answer = rag_chain.invoke(query)

        print("ğŸ§  Answer:")
        print(answer.content)

        print("\nğŸ“š Sources:")
        for i, d in enumerate(docs):
            print(f"[{i}] DocID={d.metadata.get('doc_id')}  Chunk={d.metadata.get('chunk_index')}")


if __name__ == "__main__":
    interactive_loop()
