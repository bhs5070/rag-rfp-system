import os
import json
from typing import List, Dict

import streamlit as st
import faiss
import torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

from retriever_core import RFPRetrieverCore
from lc_custom_retriever import CustomRFPRetriever
from evaluate_generator import evaluate_one


# =========================
# CONFIG
# =========================
FAISS_INDEX_PATH = "/home/bhs1581/rag-rfp-system/chunking/vector_db/vectordb_multi_aspect_index.faiss"
CHUNK_PATH = "/home/bhs1581/rag-rfp-system/chunking/chunks/chunks_multi_aspect (1).jsonl"
DOC_ORIGINAL_BASE = "/home/bhs1581/rag-rfp-system/original_docs"   # ì›ë¬¸ í´ë”

RERANKER_MODEL_NAME = "BAAI/bge-reranker-base"
THRESHOLD = -3.3

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
if not OPENAI_API_KEY:
    st.error("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()


# =========================
# UTIL
# =========================
def clean_text(t: str):
    if t is None:
        return ""
    return (
        str(t)
        .encode("utf-8", "ignore")
        .decode("utf-8", "ignore")
        .replace("\ufffd", "")
        .strip()
    )


def load_original_doc(doc_id: str):
    """ë¬¸ì„œ ì›ë³¸ ë³´ê¸° ê¸°ëŠ¥"""
    filename = f"{doc_id}.txt"
    path = os.path.join(DOC_ORIGINAL_BASE, filename)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    return "(ì›ë¬¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"


# =========================
# CACHED LOADING
# =========================
@st.cache_resource
def load_faiss_index(path):
    return faiss.read_index(path)


@st.cache_resource
def load_chunks(path):
    texts = []
    mapping = {}
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            obj = json.loads(line)
            texts.append(obj["text"])
            mapping[i] = obj["doc_id"]
    return texts, mapping


@st.cache_resource
def load_reranker():
    tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    return tokenizer, model, device


def load_llm(model_name: str):
    """gpt-5 ê³„ì—´ì€ temperature ì§€ì› ì•ˆ í•˜ë¯€ë¡œ ë¶„ë¦¬"""

    if model_name in ["gpt-5", "gpt-5-mini"]:
        return ChatOpenAI(model=model_name)
    else:
        return ChatOpenAI(model=model_name, temperature=0.2)


@st.cache_resource
def init_core_and_retriever():
    index = load_faiss_index(FAISS_INDEX_PATH)
    chunk_texts, chunk_map = load_chunks(CHUNK_PATH)
    tokenizer, model, device = load_reranker()

    core = RFPRetrieverCore(
        faiss_index=index,
        chunk_texts=chunk_texts,
        chunk_mapping=chunk_map,
        openai_api_key=OPENAI_API_KEY,
        reranker_model=model,
        reranker_tokenizer=tokenizer,
        device=device,
    )

    retriever = CustomRFPRetriever(
        core=core,
        is_multistep=True,
        top_k=10,
    )

    return core, retriever, chunk_texts, chunk_map


# =========================
# STREAMLIT UI SETUP
# =========================
st.set_page_config(page_title="RFP RAG System", layout="wide")

st.title("ğŸ“‘ RFP ê¸°ë°˜ RAG ì‹œìŠ¤í…œ")


# ëª¨ë¸ ì„ íƒ ê¸°ëŠ¥
model_name = st.sidebar.selectbox(
    "LLM ëª¨ë¸ ì„ íƒ",
    ["gpt-5-mini", "gpt-5", "gpt-4o-mini"],
    index=0,
)


core, retriever, chunk_texts, chunk_map = init_core_and_retriever()
llm = load_llm(model_name)


# RAG Prompt
prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ í•œêµ­ RFP ë¬¸ì„œ ê¸°ë°˜ QA ëª¨ë¸ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì•„ë˜ contextë§Œ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

<context>
{context}
</context>

ì§ˆë¬¸: {question}

ë¬¸ì„œì— ì •ë³´ê°€ ì—†ë‹¤ë©´ ì¶œë ¥:
"í•´ë‹¹ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
""")

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)


# =========================
# CHAT UI
# =========================
if "messages" not in st.session_state:
    st.session_state.messages = []


st.subheader("ğŸ’¬ RAG ê¸°ë°˜ Chat Interface")


# ê¸°ì¡´ ë©”ì‹œì§€ ë Œë”ë§
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])


# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if user_input:
    q = clean_text(user_input)

    st.session_state.messages.append({"role": "user", "content": q})
    st.chat_message("user").write(q)

    # RAG Retrieval
    with st.spinner("ğŸ” ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):

        docs = retriever._get_relevant_documents(q)
        rerank_score = core.last_rerank_score

        if len(docs) == 0 or rerank_score < THRESHOLD:
            answer_text = "í•´ë‹¹ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            answer_obj = rag_chain.invoke(q)
            answer_text = clean_text(answer_obj.content)

    st.session_state.messages.append({"role": "assistant", "content": answer_text})
    st.chat_message("assistant").write(answer_text)

    # ë¬¸ì„œ ì›ë¬¸ ë³´ê¸° ê¸°ëŠ¥
    if st.checkbox("ğŸ“„ ë¬¸ì„œ ì›ë¬¸ ë³´ê¸°"):
        for d in docs:
            doc_id = d.metadata.get("doc_id")
            st.markdown(f"### ğŸ“˜ DocID: {doc_id}")
            st.write(load_original_doc(doc_id))


# =========================
# Optional: í•œ ì§ˆë¬¸ í‰ê°€ ê¸°ëŠ¥
# =========================
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“Š Generator í‰ê°€ (/eval one)")

eval_q = st.sidebar.text_input("í‰ê°€ ì§ˆë¬¸ ì…ë ¥")

if st.sidebar.button("ğŸ§ª ì‹¤í–‰") and eval_q.strip():
    q = clean_text(eval_q)
    docs = retriever._get_relevant_documents(q)

    if len(docs) == 0:
        answer_text = "í•´ë‹¹ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    else:
        answer_obj = rag_chain.invoke(q)
        answer_text = clean_text(answer_obj.content)

    st.sidebar.write("### ğŸ§  Answer")
    st.sidebar.write(answer_text)

    st.sidebar.write("### ğŸ“š Sources")
    for d in docs:
        st.sidebar.write(f"- {d.metadata.get('doc_id')} / chunk {d.metadata.get('chunk_index')}")

    st.sidebar.write("### ğŸ“Š í‰ê°€ ê²°ê³¼")
    result = evaluate_one(q, answer_text, docs)
    st.sidebar.json(result)
