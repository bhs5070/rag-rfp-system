"""
Clean Eval Style Dataset Creation Module
=======================================
ìš°ë¦¬ RFP ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ Clean Eval ìŠ¤íƒ€ì¼ í‰ê°€ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ëª¨ë“ˆ

Features:
- Clean Eval ë°ì´í„°ì…‹ê³¼ ë™ì¼í•œ êµ¬ì¡° (id, question, answer, gt_doc_id)
- OpenAI GPT-4o-minië¥¼ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ ìƒì„±
- ê³ í’ˆì§ˆ ì²­í¬ ì„ ë³„ ë° ë¬¸ì„œë³„ ê· ë“± ë¶„ë°°
- RFP ë„ë©”ì¸ íŠ¹í™” ì§ˆë¬¸ íŒ¨í„´

Author: ì›í›„ (Bidding Mate RAG Team)
"""

import json
import os
import random
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm
from openai import OpenAI


class CleanEvalDatasetGenerator:
    """Clean Eval ìŠ¤íƒ€ì¼ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Args:
            api_key: OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
        """
        self.client = OpenAI(api_key=api_key)
        self.chunk_files = [
            "efficient_chunks_structure_aware.jsonl",
            # í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ì²­í‚¹ íŒŒì¼ë“¤ë„ ì¶”ê°€ ê°€ëŠ¥
        ]
        
        # ê³ í’ˆì§ˆ ì²­í¬ ì„ ë³„ ê¸°ì¤€
        self.min_text_length = 300
        self.required_keywords = ['ì‚¬ì—…', 'ì‹œìŠ¤í…œ', 'ì˜ˆì‚°', 'ê¸°ê°„', 'ìš”êµ¬ì‚¬í•­', 'êµ¬ì¶•', 'ê°œë°œ']
        self.skip_keywords = ['[í‘œ]', '[ê·¸ë¦¼]', 'ë¹ˆ í˜ì´ì§€', '[ì´ë¯¸ì§€]']
        self.chunks_per_doc = 3  # ë¬¸ì„œë‹¹ ì„ íƒí•  ì²­í¬ ìˆ˜
        
        print("âœ… Clean Eval ë°ì´í„°ì…‹ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ¤– OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ë¨")
        print(f"   ğŸ“¦ ì²­í¬ ì†ŒìŠ¤: {len(self.chunk_files)}ê°œ íŒŒì¼")
    
    def load_chunks(self) -> List[Dict]:
        """ì²­í‚¹ëœ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ ì²­í‚¹ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        chunks = []
        for chunk_file in self.chunk_files:
            if os.path.exists(chunk_file):
                print(f"   ğŸ“„ ë¡œë”©: {chunk_file}")
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    for line_no, line in enumerate(f, 1):
                        try:
                            chunk = json.loads(line)
                            chunks.append(chunk)
                        except json.JSONDecodeError as e:
                            print(f"   âš ï¸ JSON ì˜¤ë¥˜ (ë¼ì¸ {line_no}): {e}")
                            continue
            else:
                print(f"   âŒ íŒŒì¼ ì—†ìŒ: {chunk_file}")
        
        print(f"âœ… ì´ {len(chunks):,}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
        return chunks
    
    def filter_high_quality_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """ê³ í’ˆì§ˆ ì²­í¬ ì„ ë³„"""
        print("ğŸ” ê³ í’ˆì§ˆ ì²­í¬ ì„ ë³„ ì¤‘...")
        
        selected_chunks = []
        
        for chunk in chunks:
            text = chunk.get('text', '').strip()
            
            # ê¸¸ì´ ì¡°ê±´
            if len(text) < self.min_text_length:
                continue
            
            # í‚¤ì›Œë“œ ì¡°ê±´ (í•˜ë‚˜ ì´ìƒ í¬í•¨)
            if not any(keyword in text for keyword in self.required_keywords):
                continue
            
            # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            if any(skip_word in text for skip_word in self.skip_keywords):
                continue
            
            # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ ì¶”ê°€ ê²€ì¦
            if self._is_meaningful_content(text):
                selected_chunks.append(chunk)
        
        print(f"âœ… {len(selected_chunks):,}ê°œ ê³ í’ˆì§ˆ ì²­í¬ ì„ ë³„ ì™„ë£Œ")
        return selected_chunks
    
    def _is_meaningful_content(self, text: str) -> bool:
        """ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ íŒë‹¨"""
        # ë„ˆë¬´ ë°˜ë³µì ì¸ ë‚´ìš© ì œì™¸
        words = text.split()
        if len(set(words)) / len(words) < 0.3:  # ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨ì´ 30% ë¯¸ë§Œ
            return False
        
        # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if len([c for c in text if c.isalpha()]) < len(text) * 0.5:
            return False
        
        return True
    
    def balance_chunks_by_document(self, chunks: List[Dict]) -> List[Dict]:
        """ë¬¸ì„œë³„ë¡œ ê· ë“±í•˜ê²Œ ì²­í¬ ì„ íƒ"""
        print("âš–ï¸ ë¬¸ì„œë³„ ê· ë“± ë¶„ë°° ì¤‘...")
        
        # ë¬¸ì„œë³„ ê·¸ë£¹í™”
        doc_chunks = {}
        for chunk in chunks:
            doc_id = chunk.get('doc_id', 'unknown')
            if doc_id not in doc_chunks:
                doc_chunks[doc_id] = []
            doc_chunks[doc_id].append(chunk)
        
        print(f"   ğŸ“Š ì´ {len(doc_chunks)}ê°œ ë¬¸ì„œ")
        
        # ê° ë¬¸ì„œì—ì„œ ê· ë“±í•˜ê²Œ ì„ íƒ
        final_chunks = []
        for doc_id, doc_chunk_list in doc_chunks.items():
            # í’ˆì§ˆ ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸¸ì´ ê¸°ì¤€)
            doc_chunk_list.sort(key=lambda x: len(x.get('text', '')), reverse=True)
            
            # ìƒìœ„ Nê°œ ì„ íƒ
            selected_count = min(self.chunks_per_doc, len(doc_chunk_list))
            selected_from_doc = doc_chunk_list[:selected_count]
            
            final_chunks.extend(selected_from_doc)
            print(f"   ğŸ“„ {doc_id[:30]}...: {selected_count}ê°œ ì„ íƒ")
        
        print(f"âœ… ì´ {len(final_chunks)}ê°œ ì²­í¬ ìµœì¢… ì„ íƒ")
        return final_chunks
    
    def generate_question_and_answer(self, chunk_text: str, doc_id: str) -> Tuple[Optional[str], Optional[str]]:
        """ì²­í¬ ê¸°ë°˜ìœ¼ë¡œ Clean Eval ìŠ¤íƒ€ì¼ ì§ˆë¬¸ê³¼ ë‹µë³€ ìƒì„±"""
        
        question_prompt = f"""ë‹¤ìŒ RFP ë¬¸ì„œ ë‚´ìš©ì„ ë³´ê³ , Clean Eval ë°ì´í„°ì…‹ ìŠ¤íƒ€ì¼ì˜ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ: {doc_id}
ë‚´ìš©: {chunk_text[:600]}

Clean Eval ìŠ¤íƒ€ì¼ ì§ˆë¬¸ íŠ¹ì§•:
- êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ì¸ ì •ë³´ë¥¼ ë¬»ëŠ” ì§ˆë¬¸
- "ë¬´ì—‡ì¸ê°€?", "ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "ì–¼ë§ˆì¸ê°€?" ë“± ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
- ë³µí•© ì§ˆë¬¸ë„ ê°€ëŠ¥ (ì˜ˆ: ì‚¬ì—…ëª…ê³¼ ì˜ˆì‚°ì„ í•¨ê»˜ ë¬»ëŠ” ë“±)
- ì‹¤ì œ ì—…ë¬´ì—ì„œ í•„ìš”í•œ í•µì‹¬ ì •ë³´ ì¤‘ì‹¬

ì˜ˆì‹œ ì§ˆë¬¸ íŒ¨í„´:
- "ì´ ì‚¬ì—…ì˜ ê³µì‹ ëª…ì¹­ì€ ë¬´ì—‡ì¸ê°€?"
- "ë³¸ í”„ë¡œì íŠ¸ì˜ ì‚¬ì—…ê¸°ê°„ê³¼ ì˜ˆì‚° ê·œëª¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
- "ì‹œìŠ¤í…œ êµ¬ì¶•ì— í•„ìš”í•œ ì£¼ìš” ê¸°ìˆ  ìš”êµ¬ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€?"

ë¬¸ì„œ ë‚´ìš©ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ 1ê°œë§Œ ìƒì„±:"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": question_prompt}],
                max_tokens=120,
                temperature=0.7
            )
            
            question = response.choices[0].message.content.strip()
            
            # ì§ˆë¬¸ í˜•íƒœ ê²€ì¦
            if not self._validate_question(question):
                print(f"âš ï¸ ì§ˆë¬¸ í˜•íƒœ ë¶€ì ì ˆ: {question[:50]}...")
                return None, None
            
            # ë‹µë³€ì€ ì›ë³¸ ì²­í¬ í…ìŠ¤íŠ¸ ì‚¬ìš© (Clean Eval ìŠ¤íƒ€ì¼)
            answer = chunk_text.strip()
            
            return question, answer
            
        except Exception as e:
            print(f"âŒ LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return None, None
    
    def _validate_question(self, question: str) -> bool:
        """ìƒì„±ëœ ì§ˆë¬¸ì´ ì ì ˆí•œì§€ ê²€ì¦"""
        if not question:
            return False
        
        # ì§ˆë¬¸ í˜•íƒœ ì²´í¬
        if not ("?" in question or "ì¸ê°€" in question or question.endswith("ìš”?")):
            return False
        
        # ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(question.strip()) < 10:
            return False
        
        # ì ì ˆí•œ í‚¤ì›Œë“œ í¬í•¨ ì²´í¬
        question_keywords = ["ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì–¼ë§ˆ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€", "ì™œ"]
        if not any(keyword in question for keyword in question_keywords):
            return False
        
        return True
    
    def create_evaluation_dataset(self, output_file: str = "our_clean_eval_style.jsonl") -> List[Dict]:
        """Clean Eval ìŠ¤íƒ€ì¼ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±"""
        print("ğŸš€ Clean Eval ìŠ¤íƒ€ì¼ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
        print("=" * 60)
        
        # 1. ì²­í‚¹ ë°ì´í„° ë¡œë“œ
        chunks = self.load_chunks()
        if not chunks:
            print("âŒ ì²­í‚¹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 2. ê³ í’ˆì§ˆ ì²­í¬ ì„ ë³„
        high_quality_chunks = self.filter_high_quality_chunks(chunks)
        if not high_quality_chunks:
            print("âŒ ì í•©í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 3. ë¬¸ì„œë³„ ê· ë“± ë¶„ë°°
        final_chunks = self.balance_chunks_by_document(high_quality_chunks)
        
        # 4. ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±
        print("ğŸ§  ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„± ì¤‘...")
        eval_dataset = []
        failed_count = 0
        
        for idx, chunk in enumerate(tqdm(final_chunks, desc="í‰ê°€ ë°ì´í„° ìƒì„±"), 1):
            question, answer = self.generate_question_and_answer(
                chunk.get('text', ''), 
                chunk.get('doc_id', 'unknown')
            )
            
            if question and answer:
                eval_item = {
                    "id": f"eval_{idx:03d}",
                    "question": question,
                    "answer": answer,
                    "gt_doc_id": chunk.get('doc_id', 'unknown')
                }
                eval_dataset.append(eval_item)
            else:
                failed_count += 1
        
        print(f"âœ… ì§ˆë¬¸-ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(eval_dataset)}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        
        # 5. ë°ì´í„°ì…‹ ì €ì¥
        self.save_dataset(eval_dataset, output_file)
        
        # 6. í†µê³„ ì¶œë ¥
        self.print_dataset_statistics(eval_dataset)
        
        return eval_dataset
    
    def save_dataset(self, eval_dataset: List[Dict], output_file: str) -> None:
        """ë°ì´í„°ì…‹ ì €ì¥"""
        print(f"ğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in eval_dataset:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"âœ… {len(eval_dataset)}ê°œ í‰ê°€ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    
    def print_dataset_statistics(self, eval_dataset: List[Dict]) -> None:
        """ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥"""
        if not eval_dataset:
            return
        
        print("\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„")
        print("-" * 40)
        print(f"ì´ QA ìŒ: {len(eval_dataset)}ê°œ")
        
        # ë¬¸ì„œë³„ ë¶„í¬
        doc_distribution = {}
        for item in eval_dataset:
            doc_id = item['gt_doc_id']
            doc_distribution[doc_id] = doc_distribution.get(doc_id, 0) + 1
        
        print(f"ê³ ìœ  ë¬¸ì„œ: {len(doc_distribution)}ê°œ")
        print(f"ë¬¸ì„œë‹¹ í‰ê·  ì§ˆë¬¸: {len(eval_dataset)/len(doc_distribution):.1f}ê°œ")
        
        print(f"\nğŸ“„ ë¬¸ì„œë³„ ì§ˆë¬¸ ë¶„í¬:")
        for doc_id, count in sorted(doc_distribution.items(), key=lambda x: x[1], reverse=True):
            print(f"   {doc_id[:35]}...: {count}ê°œ")
        
        # ì§ˆë¬¸ ê¸¸ì´ í†µê³„
        question_lengths = [len(item['question']) for item in eval_dataset]
        print(f"\nğŸ“ ì§ˆë¬¸ ê¸¸ì´ í†µê³„:")
        print(f"   í‰ê· : {sum(question_lengths)/len(question_lengths):.1f}ì")
        print(f"   ìµœëŒ€: {max(question_lengths)}ì")
        print(f"   ìµœì†Œ: {min(question_lengths)}ì")
        
        # ë‹µë³€ ê¸¸ì´ í†µê³„
        answer_lengths = [len(item['answer']) for item in eval_dataset]
        print(f"\nğŸ’¬ ë‹µë³€ ê¸¸ì´ í†µê³„:")
        print(f"   í‰ê· : {sum(answer_lengths)/len(answer_lengths):.0f}ì")
        print(f"   ìµœëŒ€: {max(answer_lengths):,}ì")
        print(f"   ìµœì†Œ: {min(answer_lengths)}ì")
        
        # ìƒ˜í”Œ ì§ˆë¬¸ ì¶œë ¥
        print(f"\nğŸ” ìƒ˜í”Œ ì§ˆë¬¸ (ì²˜ìŒ 3ê°œ):")
        for i, item in enumerate(eval_dataset[:3], 1):
            print(f"   {i}. {item['question']}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Clean Eval ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ ìƒì„± ì‹œìŠ¤í…œ ì‹¤í–‰")
    print("=" * 60)
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   export OPENAI_API_KEY='your-api-key'")
        return
    
    # ë°ì´í„°ì…‹ ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = CleanEvalDatasetGenerator(api_key=api_key)
    
    # í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
    dataset = generator.create_evaluation_dataset()
    
    if dataset:
        print(f"\nğŸ‰ Clean Eval ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
        print(f"   ğŸ“„ íŒŒì¼: our_clean_eval_style.jsonl")
        print(f"   ğŸ“Š ì´ í‰ê°€ ìŒ: {len(dataset)}ê°œ")
        print(f"\nâœ… ì´ì œ 4ê°€ì§€ ì²­í‚¹ ì „ëµ ì„±ëŠ¥ í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ ë°ì´í„°ì…‹ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
