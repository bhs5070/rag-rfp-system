"""
Multi-Aspect Chunking Module
============================
RFP ë¬¸ì„œë¥¼ ë‹¤ê°ë„ë¡œ í‘œí˜„í•˜ëŠ” ì²­í‚¹ ì „ëµ êµ¬í˜„

í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¥¼ 3ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í‘œí˜„:
- ì›ë¬¸ (original): ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ
- í‚¤ì›Œë“œ (keywords): í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
- ìš”ì•½ (summary): ë‚´ìš© ìš”ì•½

Author: ì›í›„ (Bidding Mate RAG Team)
"""

import json
import re
from typing import List, Dict, Tuple
from tqdm import tqdm
import os
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np


class MultiAspectChunker:
    """Multi-Aspect ì²­í‚¹ í´ë˜ìŠ¤ - í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ê°ë„ë¡œ í‘œí˜„"""
    
    def __init__(self, chunk_size: int = 600, overlap: int = 150):
        """
        Args:
            chunk_size: ê¸°ë³¸ ì²­í¬ í¬ê¸°
            overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        
        # RFP ë„ë©”ì¸ ì¤‘ìš” í‚¤ì›Œë“œë“¤
        self.rfp_keywords = {
            'ê¸°ìˆ ': ['ì‹œìŠ¤í…œ', 'í”Œë«í¼', 'ì†Œí”„íŠ¸ì›¨ì–´', 'API', 'ë°ì´í„°ë² ì´ìŠ¤', 'ì„œë²„', 'í´ë¼ìš°ë“œ'],
            'ì‚¬ì—…': ['ì‚¬ì—…', 'í”„ë¡œì íŠ¸', 'êµ¬ì¶•', 'ê°œë°œ', 'ìš´ì˜', 'ìœ ì§€ë³´ìˆ˜', 'ë‚©í’ˆ'],
            'ìš”êµ¬ì‚¬í•­': ['ìš”êµ¬ì‚¬í•­', 'ê¸°ëŠ¥', 'ì„±ëŠ¥', 'ë³´ì•ˆ', 'í‘œì¤€', 'ê·œê²©', 'í’ˆì§ˆ'],
            'ì˜ˆì‚°': ['ì˜ˆì‚°', 'ë¹„ìš©', 'ê¸ˆì•¡', 'ê³„ì•½', 'ì‚¬ì—…ë¹„', 'ì´ì•¡', 'ë‹¨ê°€'],
            'ì¼ì •': ['ê¸°ê°„', 'ì¼ì •', 'ì™„ë£Œ', 'ë‚©ê¸°', 'ë‹¨ê³„', 'ë§ˆì¼ìŠ¤í†¤'],
            'í‰ê°€': ['í‰ê°€', 'ì‹¬ì‚¬', 'ì„ ì •', 'ê¸°ì¤€', 'ë°°ì ', 'ê°€ì ', 'ì ìˆ˜']
        }
        
        print("âœ… Multi-Aspect ì²­í‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def split_text_with_overlap(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """ê²¹ì¹¨ì´ ìˆëŠ” í…ìŠ¤íŠ¸ ë¶„í• """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end < len(text):
                sentence_end = text.rfind('.', start, end)
                if sentence_end > start + chunk_size * 0.7:
                    end = sentence_end + 1
                else:
                    # ì¤„ë°”ê¿ˆì—ì„œ ìë¥´ê¸° ì‹œë„
                    line_end = text.rfind('\n', start, end)
                    if line_end > start + chunk_size * 0.7:
                        end = line_end + 1
            
            chunk = text[start:end].strip()
            if chunk and len(chunk) > 50:
                chunks.append(chunk)
            
            if end >= len(text):
                break
                
            start = end - overlap
        
        return chunks
    
    def extract_keywords(self, text: str, top_k: int = 8) -> str:
        """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            words = re.findall(r'\b[ê°€-í£]{2,}\b', text)
            
            # RFP ë„ë©”ì¸ í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
            domain_keywords = []
            for category, keywords in self.rfp_keywords.items():
                for keyword in keywords:
                    if keyword in text:
                        domain_keywords.append(keyword)
            
            # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            word_freq = {}
            for word in words:
                if len(word) > 1:
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
            frequent_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            
            # ë„ë©”ì¸ í‚¤ì›Œë“œ + ë¹ˆë„ í‚¤ì›Œë“œ ì¡°í•©
            all_keywords = domain_keywords[:3]
            for word, _ in frequent_keywords:
                if word not in all_keywords and len(all_keywords) < top_k:
                    all_keywords.append(word)
            
            return ', '.join(all_keywords[:top_k]) if all_keywords else text[:100]
            
        except Exception as e:
            print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return text[:100]
    
    def summarize_text(self, text: str, max_length: int = 200) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½ (ê°„ë‹¨í•œ ì¶”ì¶œ ìš”ì•½)"""
        try:
            sentences = re.split(r'[.!?]\s+', text)
            sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
            
            if len(sentences) <= 2:
                return text[:max_length]
            
            # ì¤‘ìš” ë¬¸ì¥ ì„ íƒ (ê¸¸ì´, í‚¤ì›Œë“œ í¬í•¨ ê¸°ì¤€)
            scored_sentences = []
            for sentence in sentences:
                score = len(sentence)  # ê¸°ë³¸ ì ìˆ˜ëŠ” ê¸¸ì´
                
                # RFP ë„ë©”ì¸ í‚¤ì›Œë“œ í¬í•¨ ì‹œ ê°€ì 
                for category, keywords in self.rfp_keywords.items():
                    for keyword in keywords:
                        if keyword in sentence:
                            score += 50
                
                scored_sentences.append((sentence, score))
            
            # ìƒìœ„ ë¬¸ì¥ë“¤ ì„ íƒ
            scored_sentences.sort(key=lambda x: x[1], reverse=True)
            
            summary_sentences = []
            current_length = 0
            
            for sentence, _ in scored_sentences:
                if current_length + len(sentence) <= max_length:
                    summary_sentences.append(sentence)
                    current_length += len(sentence)
                else:
                    break
            
            if not summary_sentences:
                return text[:max_length]
            
            return '. '.join(summary_sentences) + '.'
            
        except Exception as e:
            print(f"ìš”ì•½ ì˜¤ë¥˜: {e}")
            return text[:max_length]
    
    def create_multi_aspect_chunks(self, documents: List[Dict]) -> List[Dict]:
        """Multi-Aspect ì²­í‚¹ ì‹¤í–‰"""
        print(f"Multi-Aspect ì²­í‚¹ ì‹œì‘ - {len(documents)}ê°œ ë¬¸ì„œ")
        
        all_chunks = []
        
        for doc in tqdm(documents, desc="Multi-Aspect ì²­í‚¹"):
            doc_id = doc['filename'].replace('pdf_files/', '').replace('.pdf', '')
            text = doc['text'].strip()
            
            if len(text) < 100:
                continue
            
            # ê¸°ë³¸ ì²­í¬ ë¶„í• 
            base_chunks = self.split_text_with_overlap(text, self.chunk_size, self.overlap)
            
            # ê° ì²­í¬ë¥¼ 3ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
            for chunk_idx, chunk_text in enumerate(base_chunks):
                # 1. ì›ë¬¸ (Original)
                original_chunk = {
                    "chunk_id": f"{doc_id}_multi_original_{chunk_idx}",
                    "doc_id": doc_id,
                    "aspect": "original",
                    "text": f"ì›ë¬¸: {chunk_text}",
                    "metadata": {
                        "filename": doc['filename'],
                        "chunk_type": "multi_aspect",
                        "aspect_type": "original",
                        "chunk_index": chunk_idx,
                        "original_length": len(chunk_text)
                    }
                }
                all_chunks.append(original_chunk)
                
                # 2. í‚¤ì›Œë“œ (Keywords)
                keywords = self.extract_keywords(chunk_text)
                keyword_chunk = {
                    "chunk_id": f"{doc_id}_multi_keywords_{chunk_idx}",
                    "doc_id": doc_id,
                    "aspect": "keywords", 
                    "text": f"í‚¤ì›Œë“œ: {keywords}",
                    "metadata": {
                        "filename": doc['filename'],
                        "chunk_type": "multi_aspect",
                        "aspect_type": "keywords",
                        "chunk_index": chunk_idx,
                        "original_text": chunk_text
                    }
                }
                all_chunks.append(keyword_chunk)
                
                # 3. ìš”ì•½ (Summary)
                summary = self.summarize_text(chunk_text)
                summary_chunk = {
                    "chunk_id": f"{doc_id}_multi_summary_{chunk_idx}",
                    "doc_id": doc_id,
                    "aspect": "summary",
                    "text": f"ìš”ì•½: {summary}",
                    "metadata": {
                        "filename": doc['filename'],
                        "chunk_type": "multi_aspect", 
                        "aspect_type": "summary",
                        "chunk_index": chunk_idx,
                        "original_text": chunk_text
                    }
                }
                all_chunks.append(summary_chunk)
        
        print(f"Multi-Aspect ì²­í‚¹ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
        print(f"ì²­í¬ êµ¬ì„±: ì›ë¬¸({len(all_chunks)//3}ê°œ), í‚¤ì›Œë“œ({len(all_chunks)//3}ê°œ), ìš”ì•½({len(all_chunks)//3}ê°œ)")
        
        return all_chunks
    
    def save_chunks(self, chunks: List[Dict], output_file: str) -> None:
        """ì²­í¬ ê²°ê³¼ ì €ì¥"""
        print(f"ì²­í¬ ì €ì¥: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for chunk in chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
        
        print(f"{len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
    
    def load_extracted_texts(self, jsonl_file: str) -> List[Dict]:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¡œë“œ"""
        print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ: {jsonl_file}")
        
        documents = []
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                documents.append(data)
        
        print(f"{len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return documents


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Multi-Aspect ì²­í‚¹ ì‹œìŠ¤í…œ ì‹¤í–‰")
    print("="*50)
    
    # ì²­í‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    chunker = MultiAspectChunker(chunk_size=600, overlap=150)
    
    # í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    input_file = "all_extracted_texts.jsonl"  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
    
    if not os.path.exists(input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        print("   í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    documents = chunker.load_extracted_texts(input_file)
    
    # Multi-Aspect ì²­í‚¹ ì‹¤í–‰
    chunks = chunker.create_multi_aspect_chunks(documents)
    
    # ê²°ê³¼ ì €ì¥
    output_file = "multi_aspect_chunks.jsonl"
    chunker.save_chunks(chunks, output_file)
    
    # í†µê³„ ì¶œë ¥
    print("\nMulti-Aspect ì²­í‚¹ ê²°ê³¼ í†µê³„")
    print("-" * 30)
    print(f"ì´ ì²­í¬ ìˆ˜: {len(chunks):,}ê°œ")
    
    # ì¸¡ë©´ë³„ í†µê³„
    aspect_counts = {}
    for chunk in chunks:
        aspect = chunk['aspect']
        aspect_counts[aspect] = aspect_counts.get(aspect, 0) + 1
    
    for aspect, count in aspect_counts.items():
        print(f"{aspect}: {count:,}ê°œ")
    
    print(f"\nMulti-Aspect ì²­í‚¹ ì™„ë£Œ!")
    print(f"   ì¶œë ¥ íŒŒì¼: {output_file}")


if __name__ == "__main__":
    main()